{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5420e5f7-7e1a-432a-a69e-f72fc51af46f",
   "metadata": {},
   "source": [
    "# FCN for Dendrites Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e1e37-9c27-46e6-87ea-186330ce46fc",
   "metadata": {},
   "source": [
    "### Set Proxy Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da593b-f731-417a-ac2b-02a952ed89b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set proxy environment variables\n",
    "os.environ['http_proxy'] = 'http://proxy:80'\n",
    "os.environ['https_proxy'] = 'http://proxy:80'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6c7cf-222a-4279-956b-60ce89b55af0",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f345d8-9219-43f4-b690-87d33aceacc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from torchvision.models.segmentation import FCN_ResNet50_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c9e39e-9d10-45dc-ba98-915b7a860a85",
   "metadata": {},
   "source": [
    "### Define Custom Dendrite Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645f3ef-4f59-477d-8513-289dec1a43cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DendritesDataset(Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.images = sorted([f for f in os.listdir(os.path.join(root, \"input_images\")) if os.path.isfile(os.path.join(root, \"input_images\", f))])\n",
    "        self.masks = sorted([f for f in os.listdir(os.path.join(root, \"dendrite_images\")) if os.path.isfile(os.path.join(root, \"dendrite_images\", f))])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"input_images\", self.images[idx])\n",
    "        mask_path = os.path.join(self.root, \"dendrite_images\", self.masks[idx])\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "            mask = self.transforms(mask)\n",
    "            mask = (mask > 0).float()\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fba2bc-9167-4dc5-9747-990ef75e73db",
   "metadata": {},
   "source": [
    "### Define Transformations for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b35aae9-aff6-4d84-9892-fa851260796f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd4f9c-7c75-421c-98bc-ed90d2197344",
   "metadata": {},
   "source": [
    "### Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef04e60-b73a-4989-9df6-9063ecd87233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_train = 'Dataset/DeepD3_Training'\n",
    "root_val = 'Dataset/DeepD3_Validation'\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    DendritesDataset(root_train, transforms=train_transform), \n",
    "    batch_size=4, \n",
    "    shuffle=True, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    DendritesDataset(root_val, transforms=val_transform), \n",
    "    batch_size=4, \n",
    "    shuffle=False, \n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146f700-6b0b-4f30-aafe-b58a8521d1a2",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50335d-81a1-40b7-b554-cbd4cc77dfca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model with the weights\n",
    "weights = FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\n",
    "model = fcn_resnet50(weights=weights, progress=True)\n",
    "model.classifier[4] = nn.Conv2d(512, 1, kernel_size=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5497bf-d611-4f5b-b4a0-87f8945659a1",
   "metadata": {},
   "source": [
    "### Metrics for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be168fc1-a19a-494d-b026-12ccfa2239c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(outputs, masks):\n",
    "    outputs = torch.sigmoid(outputs)\n",
    "    outputs = (outputs > 0.5).float()\n",
    "\n",
    "    outputs = outputs.cpu().detach().numpy().astype(np.uint8).flatten()\n",
    "    masks = masks.cpu().detach().numpy().astype(np.uint8).flatten()\n",
    "\n",
    "    accuracy = accuracy_score(masks, outputs)\n",
    "    precision = precision_score(masks, outputs, zero_division=1)\n",
    "    recall = recall_score(masks, outputs, zero_division=1)\n",
    "    \n",
    "    intersection = np.sum((masks * outputs) > 0)\n",
    "    union = np.sum((masks + outputs) > 0)\n",
    "    iou = intersection / union if union > 0 else 0.0\n",
    "    return accuracy, precision, recall, iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6b2ec0-71e9-4d56-a442-025df1df8a98",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb49e83-27f3-45de-8759-8c71b7c34e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, criterion, optimizer, epoch, num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    total_precision = 0.0\n",
    "    total_recall = 0.0\n",
    "    total_iou = 0.0\n",
    "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "    \n",
    "    for images, masks in progress_bar:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        accuracy, precision, recall, iou = calculate_metrics(outputs, masks)\n",
    "        total_accuracy += accuracy * images.size(0)\n",
    "        total_precision += precision * images.size(0)\n",
    "        total_recall += recall * images.size(0)\n",
    "        total_iou += iou * images.size(0)\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader.dataset)\n",
    "    epoch_accuracy = total_accuracy / len(data_loader.dataset)\n",
    "    epoch_precision = total_precision / len(data_loader.dataset)\n",
    "    epoch_recall = total_recall / len(data_loader.dataset)\n",
    "    epoch_iou = total_iou / len(data_loader.dataset)\n",
    "    \n",
    "    return epoch_loss, epoch_accuracy, epoch_precision, epoch_recall, epoch_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e6e97f-541e-4746-b006-328e37cc73f1",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5383ca38-fc73-4ff4-b4ae-630a1a1260f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model, criterion, epoch, num_epochs):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    total_precision = 0.0\n",
    "    total_recall = 0.0\n",
    "    total_iou = 0.0\n",
    "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\")\n",
    "    with torch.no_grad():\n",
    "        for images, masks in progress_bar:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device).float()\n",
    "\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            accuracy, precision, recall, iou = calculate_metrics(outputs, masks)\n",
    "            total_accuracy += accuracy * images.size(0)\n",
    "            total_precision += precision * images.size(0)\n",
    "            total_recall += recall * images.size(0)\n",
    "            total_iou += iou * images.size(0)\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader.dataset)\n",
    "    epoch_accuracy = total_accuracy / len(data_loader.dataset)\n",
    "    epoch_precision = total_precision / len(data_loader.dataset)\n",
    "    epoch_recall = total_recall / len(data_loader.dataset)\n",
    "    epoch_iou = total_iou / len(data_loader.dataset)\n",
    "    \n",
    "    return epoch_loss, epoch_accuracy, epoch_precision, epoch_recall, epoch_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80d499-2e1e-4a5a-be82-ec996220307f",
   "metadata": {},
   "source": [
    "### Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aa3781-9b41-4c92-9a1c-2503e8f81be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6971aa-3d74-4e51-b2ca-5169db78e9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "EPOCHS = 100\n",
    "CHECKPOINT_DIR = 'checkpoints_dendrites'\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'model_checkpoint.pth')\n",
    "METRICS_PATH = os.path.join(CHECKPOINT_DIR, 'metrics.npz')\n",
    "LOG_PATH = os.path.join(CHECKPOINT_DIR, 'metrics_log.log')\n",
    "\n",
    "# Create checkpoint directory if it doesn't exist\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize variables\n",
    "best_valid_loss = np.Inf\n",
    "metrics = {\n",
    "    'train_losses': [],\n",
    "    'valid_losses': [],\n",
    "    'train_accuracies': [],\n",
    "    'valid_accuracies': [],\n",
    "    'train_precisions': [],\n",
    "    'valid_precisions': [],\n",
    "    'train_recalls': [],\n",
    "    'valid_recalls': [],\n",
    "    'train_ious': [],\n",
    "    'valid_ious': []\n",
    "}\n",
    "\n",
    "def load_checkpoint():\n",
    "    global best_valid_loss\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_valid_loss = checkpoint['best_valid_loss']\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        if os.path.exists(METRICS_PATH):\n",
    "            loaded_metrics = np.load(METRICS_PATH)\n",
    "            for key in metrics:\n",
    "                metrics[key] = loaded_metrics[key].tolist()\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "    return start_epoch\n",
    "\n",
    "def save_checkpoint(epoch):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_valid_loss': best_valid_loss\n",
    "    }, CHECKPOINT_PATH)\n",
    "    np.savez(METRICS_PATH, **metrics)\n",
    "    \n",
    "    # Log metrics to log file\n",
    "    with open(LOG_PATH, 'a') as logfile:\n",
    "        logfile.write(f\"Epoch {epoch + 1}/{EPOCHS}\\n\")\n",
    "        logfile.write(f\"Train Loss: {metrics['train_losses'][-1]:.4f} | Acc: {metrics['train_accuracies'][-1]:.4f} | Precision: {metrics['train_precisions'][-1]:.4f} | Recall: {metrics['train_recalls'][-1]:.4f} | IoU: {metrics['train_ious'][-1]:.4f}\\n\")\n",
    "        logfile.write(f\"Valid Loss: {metrics['valid_losses'][-1]:.4f} | Acc: {metrics['valid_accuracies'][-1]:.4f} | Precision: {metrics['valid_precisions'][-1]:.4f} | Recall: {metrics['valid_recalls'][-1]:.4f} | IoU: {metrics['valid_ious'][-1]:.4f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e048dda8-c57a-4548-8b46-ed87a6a8d65f",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34f731-5dda-4e5c-833b-03f8cc11c537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_epoch = load_checkpoint()\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    train_loss, train_accuracy, train_precision, train_recall, train_iou = train_fn(train_loader, model, criterion, optimizer, epoch, EPOCHS)\n",
    "    valid_loss, valid_accuracy, valid_precision, valid_recall, valid_iou = eval_fn(val_loader, model, criterion, epoch, EPOCHS)\n",
    "\n",
    "    metrics['train_losses'].append(train_loss)\n",
    "    metrics['valid_losses'].append(valid_loss)\n",
    "    metrics['train_accuracies'].append(train_accuracy)\n",
    "    metrics['valid_accuracies'].append(valid_accuracy)\n",
    "    metrics['train_precisions'].append(train_precision)\n",
    "    metrics['valid_precisions'].append(valid_precision)\n",
    "    metrics['train_recalls'].append(train_recall)\n",
    "    metrics['valid_recalls'].append(valid_recall)\n",
    "    metrics['train_ious'].append(train_iou)\n",
    "    metrics['valid_ious'].append(valid_iou)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Acc: {train_accuracy:.4f} | Precision: {train_precision:.4f} | Recall: {train_recall:.4f} | IoU: {train_iou:.4f}\")\n",
    "    print(f\"Valid Loss: {valid_loss:.4f} | Acc: {valid_accuracy:.4f} | Precision: {valid_precision:.4f} | Recall: {valid_recall:.4f} | IoU: {valid_iou:.4f}\")\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        torch.save(model.state_dict(), 'dendrite_model.pt')\n",
    "        best_valid_loss = valid_loss\n",
    "\n",
    "    save_checkpoint(epoch)\n",
    "    lr_scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055b193b-087e-4e62-849c-c549cb2fc10a",
   "metadata": {},
   "source": [
    "### End of Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIMAP",
   "language": "python",
   "name": "bimap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
