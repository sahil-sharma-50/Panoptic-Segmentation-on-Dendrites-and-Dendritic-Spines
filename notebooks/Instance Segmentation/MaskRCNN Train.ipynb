{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157b0cfb-96e6-43f6-869b-21e818e78cbf",
   "metadata": {},
   "source": [
    "# MaskRCNN for Spines Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a19f56-73b6-4df8-ba7f-12845870f160",
   "metadata": {},
   "source": [
    "### Set Proxy Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a20126-80ad-403c-b398-eca6c04f5e88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set proxy environment variables\n",
    "os.environ['http_proxy'] = 'http://proxy:80'\n",
    "os.environ['https_proxy'] = 'http://proxy:80'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2220cfb6-84d5-400b-9ec5-771adbfff36a",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c39d8c-6a9a-47d9-826f-64178ff3fa18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1332e03-3911-4825-91c3-29ab0bf986c6",
   "metadata": {},
   "source": [
    "### Define Custom Spine Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a612c-2530-4387-84a9-0f9811c528db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SpineDataset(Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.imgs = sorted([f for f in os.listdir(os.path.join(root, \"input_images\")) if os.path.isfile(os.path.join(root, \"input_images\", f))])\n",
    "        self.masks = sorted([f for f in os.listdir(os.path.join(root, \"spine_images\")) if os.path.isfile(os.path.join(root, \"spine_images\", f))])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"input_images\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"spine_images\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        img = np.array(img)\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=img, mask=mask)\n",
    "            img = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "\n",
    "        obj_ids = np.unique(mask)[1:]\n",
    "\n",
    "        boxes, masks = [], []\n",
    "        for obj_id in obj_ids:\n",
    "            mask_obj = mask == obj_id\n",
    "            pos = np.where(mask_obj)\n",
    "            if pos[0].size > 0 and pos[1].size > 0:\n",
    "                xmin, xmax = np.min(pos[1]), np.max(pos[1])\n",
    "                ymin, ymax = np.min(pos[0]), np.max(pos[0])\n",
    "                if xmax > xmin and ymax > ymin:\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    masks.append(mask_obj)\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            masks = torch.zeros((0, mask.shape[0], mask.shape[1]), dtype=torch.uint8)\n",
    "            area = torch.zeros((0,), dtype=torch.float32)\n",
    "            iscrowd = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.ones((len(boxes),), dtype=torch.int64)\n",
    "            masks = np.array(masks, dtype=np.uint8)\n",
    "            masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": masks,\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": iscrowd\n",
    "        }\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53fcc46-5105-42e1-a5c7-51f9b6a19890",
   "metadata": {},
   "source": [
    "### Define Transformations for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ac91c-6b6d-498c-ba80-08635ccc6e72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "train_transform = A.Compose([\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142db5a-3db6-4812-9a30-253c20c6a6af",
   "metadata": {},
   "source": [
    "### Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0048a4e-bcfe-46d9-8b30-71fdd8046cf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Dataset and DataLoader\n",
    "root_train = 'Dataset/DeepD3_Training'\n",
    "root_val = 'Dataset/DeepD3_Validation'\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    SpineDataset(root_train, transforms=train_transform), \n",
    "    batch_size=1, \n",
    "    shuffle=True, \n",
    "    num_workers=2, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    SpineDataset(root_val, transforms=val_transform), \n",
    "    batch_size=1, \n",
    "    shuffle=False, \n",
    "    num_workers=2, \n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fab4b45-f25d-4082-9cc4-d4ace01d3e35",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2af9f3-3b58-4845-83ac-c2a3592c6bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model initialization function\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    weights = MaskRCNN_ResNet50_FPN_Weights.COCO_V1\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=weights)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 512\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    return model\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = 2  # Background and spine\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "model.to(device)\n",
    "print('Model Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7725ea60-9e08-48ee-9572-538c8d78266c",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865595d8-f293-4cd2-a0be-ab2eb5daac8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, num_epochs):\n",
    "    model.train()\n",
    "    train_epoch_loss = 0\n",
    "    loss_components = {'loss_box_reg': 0, 'loss_classifier': 0, 'loss_mask': 0}\n",
    "    progress_bar = tqdm.tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "\n",
    "    for images, targets in progress_bar:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_epoch_loss += losses.item()\n",
    "        for k in loss_components.keys():\n",
    "            if k in loss_dict:\n",
    "                loss_components[k] += loss_dict[k].item()\n",
    "\n",
    "        progress_bar.set_postfix(loss=losses.item())\n",
    "\n",
    "    num_batches = len(data_loader)\n",
    "    avg_loss_components = {k: v / num_batches for k, v in loss_components.items()}\n",
    "    return train_epoch_loss / num_batches, avg_loss_components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75ea8ca-1468-4102-963c-3226af0267d7",
   "metadata": {},
   "source": [
    "### Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3917c722-07f9-4d80-81de-98b410de786c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(model, images, targets):\n",
    "    model.train()\n",
    "    loss_dict = model(images, targets)\n",
    "    losses = sum(loss for loss in loss_dict.values())\n",
    "    model.eval()\n",
    "    return loss_dict, losses\n",
    "\n",
    "def validate_one_epoch(model, data_loader, device, epoch, num_epochs):\n",
    "    model.eval()\n",
    "    val_epoch_loss = 0\n",
    "    loss_components = {'loss_box_reg': 0, 'loss_classifier': 0, 'loss_mask': 0}\n",
    "    val_progress_bar = tqdm.tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_progress_bar:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            loss_dict, losses = compute_loss(model, images, targets)\n",
    "\n",
    "            val_epoch_loss += losses.item()\n",
    "            for k in loss_components.keys():\n",
    "                if k in loss_dict:\n",
    "                    loss_components[k] += loss_dict[k].item()\n",
    "\n",
    "            val_progress_bar.set_postfix(loss=losses.item())\n",
    "\n",
    "    num_batches = len(data_loader)\n",
    "    avg_loss_components = {k: v / num_batches for k, v in loss_components.items()}\n",
    "    return val_epoch_loss / num_batches, avg_loss_components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d11d15-0219-4c38-8b6d-4af8252290d1",
   "metadata": {},
   "source": [
    "### Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6fbe4-e8cb-4b8d-8169-c074090908af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer and scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.00001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "EPOCHS = 100\n",
    "best_val_loss = np.inf\n",
    "train_losses, val_losses = [], []\n",
    "train_loss_components_list, val_loss_components_list = [], []\n",
    "\n",
    "# Checkpointing setup\n",
    "CHECKPOINT_DIR = 'checkpoint_spines'\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'checkpoint.pth')\n",
    "LOG_PATH = os.path.join(CHECKPOINT_DIR, 'training_log.log')\n",
    "LOSS_PATH = os.path.join(CHECKPOINT_DIR, 'losses.npz')\n",
    "\n",
    "# Create checkpoint directory if it doesn't exist\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "def load_checkpoint():\n",
    "    global best_val_loss\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        if os.path.exists(LOSS_PATH):\n",
    "            loaded_losses = np.load(LOSS_PATH)\n",
    "            train_losses.extend(loaded_losses['train_losses'].tolist())\n",
    "            val_losses.extend(loaded_losses['val_losses'].tolist())\n",
    "    return start_epoch\n",
    "\n",
    "def save_checkpoint(epoch):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val_loss\n",
    "    }, CHECKPOINT_PATH)\n",
    "    np.savez(LOSS_PATH, train_losses=train_losses, val_losses=val_losses)\n",
    "\n",
    "    formatted_train_loss_components = \" | \".join([f\"{k}: {v:.4f}\" for k, v in train_loss_components_list[-1].items()])\n",
    "    formatted_val_loss_components = \" | \".join([f\"{k}: {v:.4f}\" for k, v in val_loss_components_list[-1].items()])\n",
    "\n",
    "    with open(LOG_PATH, 'a') as logfile:\n",
    "        logfile.write(f\"Epoch {epoch + 1}/{EPOCHS}\\n\")\n",
    "        logfile.write(f\"Train Loss: {train_losses[-1]:.4f} | {formatted_train_loss_components}\\n\")\n",
    "        logfile.write(f\"Valid Loss: {val_losses[-1]:.4f} | {formatted_val_loss_components}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b78e6-591d-4029-82a5-2ddbb88c933c",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d252c0-5529-44c3-9d0e-7c44054f4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint if exists\n",
    "start_epoch = load_checkpoint()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    avg_train_loss, train_loss_components = train_one_epoch(model, optimizer, train_loader, device, epoch, EPOCHS)\n",
    "    avg_val_loss, val_loss_components = validate_one_epoch(model, val_loader, device, epoch, EPOCHS)\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    train_loss_components_list.append(train_loss_components)\n",
    "    val_loss_components_list.append(val_loss_components)\n",
    "\n",
    "    lr_scheduler.step(avg_val_loss)\n",
    "\n",
    "    formatted_train_loss_components = \" | \".join([f\"{k}: {v:.4f}\" for k, v in train_loss_components.items()])\n",
    "    formatted_val_loss_components = \" | \".join([f\"{k}: {v:.4f}\" for k, v in val_loss_components.items()])\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | {formatted_train_loss_components}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f} | {formatted_val_loss_components}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'spines_model.pt')\n",
    "\n",
    "    save_checkpoint(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d092f-36aa-4a2f-bd42-b8aae43876aa",
   "metadata": {},
   "source": [
    "### End of Script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIMAP",
   "language": "python",
   "name": "bimap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
